import math
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm # tqdmã‚’è¿½åŠ ã™ã‚‹ã¨é€²æ—ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã§ã™

# ======================
# Positional Encoding (å¤‰æ›´ãªã—)
# ======================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=2000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float()
                        * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer("pe", pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# ======================
# Handwriting Transformer (encodeã¨decodeã«åˆ†å‰²ã—ã€ã‚ˆã‚Šã‚¯ãƒªãƒ¼ãƒ³ã«)
# ======================
class HandwritingTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=8,
                 num_encoder_layers=4, num_decoder_layers=4,
                 dim_feedforward=512, dropout=0.1,
                 mdn_components=20, pen_states=2, stroke_dim=4): # stroke_dimå¼•æ•°ã‚’è¿½åŠ 
        super().__init__()
        self.d_model = d_model
        self.text_tok = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.text_pos = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward, dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)

        self.stroke_in = nn.Linear(stroke_dim, d_model)
        self.stroke_pos = PositionalEncoding(d_model)
        decoder_layer = nn.TransformerDecoderLayer(
            d_model, nhead, dim_feedforward, dropout, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)

        K = mdn_components
        self.mdn_pi = nn.Linear(d_model, K)
        self.mdn_mu_x = nn.Linear(d_model, K)
        self.mdn_mu_y = nn.Linear(d_model, K)
        self.mdn_sigma_x = nn.Linear(d_model, K)
        self.mdn_sigma_y = nn.Linear(d_model, K)
        self.mdn_rho = nn.Linear(d_model, K)
        self.pen_logits = nn.Linear(d_model, pen_states)

    def encode(self, text_ids, text_mask):
        te = self.text_tok(text_ids) * math.sqrt(self.d_model)
        te = self.text_pos(te)
        memory = self.encoder(te, src_key_padding_mask=~text_mask)
        return memory

    def decode(self, strokes, stroke_mask, memory, memory_mask):
        B, T, _ = strokes.shape
        di = self.stroke_in(strokes)
        di = self.stroke_pos(di)
        tgt_mask = torch.triu(torch.ones(
            (T, T), device=strokes.device), diagonal=1).bool()
        
        dec = self.decoder(tgt=di, memory=memory,
                           tgt_mask=tgt_mask,
                           memory_key_padding_mask=~memory_mask,
                           tgt_key_padding_mask=~stroke_mask)
        return dec
        
    def forward(self, text_ids, text_mask, strokes, stroke_mask):
        """å­¦ç¿’æ™‚ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆä»Šå›ã¯ä½¿ã‚ãªã„ï¼‰"""
        memory = self.encode(text_ids, text_mask)
        dec = self.decode(strokes, stroke_mask, memory, text_mask)
        # ... (æå¤±è¨ˆç®—ç”¨ã®ãƒ˜ãƒƒãƒ‰ã‚’ã“ã“ã«æ›¸ã) ...

# ======================
# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–¢æ•° (ãƒ¢ãƒ‡ãƒ«ã®encode/decodeã‚’ä½¿ã†ã‚ˆã†ã«ä¿®æ­£)
# ======================
def sample_bivariate(pi, mu_x, mu_y, sigma_x, sigma_y, rho):
    # softmaxã§ç¢ºç‡ã«å¤‰æ›ã—ã€numpyé…åˆ—ã«
    pi_probs = torch.softmax(pi, dim=-1).detach().cpu().numpy()
    # ç¢ºç‡pi_probsã«åŸºã¥ã„ã¦ã€ã©ã®æ··åˆè¦ç´ ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰ã‚’ä½¿ã†ã‹é¸æŠ
    comp_idx = np.random.choice(len(pi_probs), p=pi_probs)

    # é¸æŠã•ã‚ŒãŸã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    mean = [mu_x[comp_idx].item(), mu_y[comp_idx].item()]
    sx = sigma_x[comp_idx].item()
    sy = sigma_y[comp_idx].item()
    r = rho[comp_idx].item()
    
    # å…±åˆ†æ•£è¡Œåˆ—ã‚’ä½œæˆ
    cov = [[sx**2, r * sx * sy], [r * sx * sy, sy**2]]
    
    # 2å¤‰é‡æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    dx, dy = np.random.multivariate_normal(mean, cov)
    return dx, dy


def generate_strokes(model, vocab, text="ã‚", max_steps=200, device="cuda"):
    model.eval()
    
    # 1. ãƒ†ã‚­ã‚¹ãƒˆIDã‚’æº–å‚™ã—ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§æ–‡è„ˆ(memory)ã‚’ä¸€åº¦ã ã‘è¨ˆç®—
    text_ids = torch.tensor([[vocab[ch] for ch in text]], dtype=torch.long, device=device)
    text_mask = (text_ids != 0)
    memory = model.encode(text_ids, text_mask)

    # 2. ç”Ÿæˆãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–
    # æœ€åˆã®ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ã¯ (0,0) ã§ãƒšãƒ³ã‚’ä¸‹ã‚ã—ãŸçŠ¶æ…‹
    stroke_sequence = [torch.tensor([[0.0, 0.0, 1.0, 0.0]], dtype=torch.float32, device=device)]

    for _ in range(max_steps):
        # ç¾åœ¨ã®ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ç³»åˆ—å…¨ä½“ã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å…¥åŠ›ã¨ã™ã‚‹
        strokes_tensor = torch.cat(stroke_sequence, dim=0).unsqueeze(0) # [1, ç¾åœ¨ã®é•·ã•, 4]
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
        # stroke_maskã¯ç”Ÿæˆæ™‚ã«ã¯ä½¿ã‚ãªã„ã®ã§ãƒ€ãƒŸãƒ¼ï¼ˆã™ã¹ã¦Trueï¼‰ã‚’æ¸¡ã™
        dummy_stroke_mask = torch.ones(strokes_tensor.shape[:2], dtype=torch.bool, device=device)
        dec_out = model.decode(strokes_tensor, dummy_stroke_mask, memory, text_mask)
        
        # æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã ã‘ã‚’å–ã‚Šå‡ºã™
        last_step_out = dec_out[0, -1]

        # 3. æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‹ã‚‰MDNãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒšãƒ³ã®çŠ¶æ…‹ã‚’å–å¾—
        pi = model.mdn_pi(last_step_out)
        mu_x, mu_y = model.mdn_mu_x(last_step_out), model.mdn_mu_y(last_step_out)
        sigma_x, sigma_y = torch.exp(model.mdn_sigma_x(last_step_out)), torch.exp(model.mdn_sigma_y(last_step_out))
        rho = torch.tanh(model.mdn_rho(last_step_out))
        pen_logits = model.pen_logits(last_step_out)

        # 4. æ¬¡ã®ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        dx, dy = sample_bivariate(pi, mu_x, mu_y, sigma_x, sigma_y, rho)
        pen_probs = torch.softmax(pen_logits, dim=-1)
        pen_state = torch.multinomial(pen_probs, 1).item()
        
        # ãƒšãƒ³ã®çŠ¶æ…‹ã‚’one-hotãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        pen_onehot = [1.0, 0.0] if pen_state == 0 else [0.0, 1.0]
        
        # æ–°ã—ã„ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ã‚’è¿½åŠ 
        new_stroke = torch.tensor([[dx, dy] + pen_onehot], dtype=torch.float32, device=device)
        stroke_sequence.append(new_stroke)
        
        # ã‚‚ã—ãƒšãƒ³ãŒç´™ã‹ã‚‰é›¢ã‚ŒãŸã‚‰ï¼ˆend of strokeï¼‰ã€ç”Ÿæˆã‚’çµ‚äº†
        if pen_state == 1:
            break

    return torch.cat(stroke_sequence, dim=0).cpu().numpy()

# ======================
# ãƒ—ãƒ­ãƒƒãƒˆé–¢æ•° (å¤‰æ›´ãªã—)
# ======================
def strokes_to_xy(strokes):
    x, y = 0, 0
    xy = []
    for dx, dy, p_down, p_up in strokes:
        x += dx
        y += dy
        xy.append((x, y, p_down))
    return np.array(xy)

def plot_strokes(xy, save_path):
    plt.figure(figsize=(4, 4))
    xs, ys, ps = xy[:, 0], xy[:, 1], xy[:, 2]
    start = 0
    # ãƒšãƒ³ãŒ down (ps > 0.5) ã®é–“ã ã‘ç·šã‚’å¼•ã
    for i in range(1, len(xs)):
        if ps[i-1] > 0.5: # 1ã¤å‰ã®ç‚¹ãŒãƒšãƒ³ã‚’ä¸‹ã‚ã—ãŸçŠ¶æ…‹ãªã‚‰ç·šã‚’å¼•ã
            plt.plot(xs[i-1:i+1], ys[i-1:i+1], "k-", linewidth=2.0)
    
    plt.gca().invert_yaxis()
    plt.axis("equal")
    plt.axis('off') # è»¸ã‚’éè¡¨ç¤ºã«
    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)
    plt.close() # ãƒ—ãƒ­ãƒƒãƒˆã‚’é–‰ã˜ã‚‹

# ======================
# å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ======================
if __name__ == "__main__":
    # vocabè¾æ›¸ã®é‡è¤‡ã¨ç•ªå·ã®èª¤ã‚Šã‚’ä¿®æ­£
    vocab = {
        "ã‚": 1, "ã„": 2, "ã†": 3, "ãˆ": 4, "ãŠ": 5,
        "ã‹": 6, "ã": 7, "ã": 8, "ã‘": 9, "ã“": 10,
        "ã•": 11, "ã—": 12, "ã™": 13, "ã›": 14, "ã": 15,
        "ãŸ": 16, "ã¡": 17, "ã¤": 18, "ã¦": 19, "ã¨": 20,
        "ãª": 21, "ã«": 22, "ã¬": 23, "ã­": 24, "ã®": 25,
        "ã¯": 26, "ã²": 27, "ãµ": 28, "ã¸": 29, "ã»": 30,
        "ã¾": 31, "ã¿": 32, "ã‚€": 33, "ã‚": 34, "ã‚‚": 35,
        "ã‚„": 36, "ã‚†": 37, "ã‚ˆ": 38,
        "ã‚‰": 39, "ã‚Š": 40, "ã‚‹": 41, "ã‚Œ": 42, "ã‚": 43,
        "ã‚": 44, "ã‚’": 45, "ã‚“": 46, "ã€‚": 47, "ã€": 48,
    }
    device = "cuda" if torch.cuda.is_available() else "cpu"

    model = HandwritingTransformer(
        vocab_size=len(vocab)+1,
        d_model=256, nhead=8,
        num_encoder_layers=4, num_decoder_layers=4,
        dim_feedforward=512, mdn_components=20, pen_states=2, stroke_dim=4
    ).to(device)

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æ­£ã—ãæŒ‡å®šã—ã¦ãã ã•ã„
    checkpoint_path = "checkpoints/handwriting_epoch10.pt"
    try:
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        print(f"âœ… Model loaded from {checkpoint_path}")
    except FileNotFoundError:
        print(f"ğŸš¨ Error: Checkpoint file not found at {checkpoint_path}")
        exit() # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°çµ‚äº†

    output_folder = "generated_strokes"
    os.makedirs(output_folder, exist_ok=True)

    # vocabã®æ–‡å­—ã‚’ãƒ«ãƒ¼ãƒ—ã§å‡¦ç†
    # tqdmã§ãƒ©ãƒƒãƒ—ã™ã‚‹ã¨ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    for char in tqdm(vocab.keys(), desc="Generating Characters"):
        try:
            strokes = generate_strokes(model, vocab, text=char, max_steps=1000, device=device)
            xy = strokes_to_xy(strokes)
            
            save_path = os.path.join(output_folder, f"{char}.png")
            plot_strokes(xy, save_path)

        except Exception as e:
            print(f"Error processing '{char}': {e}")
            # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’çŸ¥ã‚ŠãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤
            # import traceback
            # traceback.print_exc()

    print(f"\nâœ… All characters have been processed and saved to '{output_folder}' folder.")